{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d969a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import sys\n",
    "import matplotlib as mpl \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm as tqdmm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from IPython.display import HTML, display,clear_output\n",
    "from collections import Counter\n",
    "\n",
    "import Utils.Dataload as d\n",
    "\n",
    "# part = 'shadow/'\n",
    "# subpart = 'copyMachine/'\n",
    "\n",
    "part = 'baseline/'\n",
    "subpart = 'office/'\n",
    "\n",
    "# part = 'cameraJitter/'\n",
    "# subpart = 'badminton/'\n",
    "\n",
    "# part = 'badWeather/'\n",
    "# subpart = 'blizzard/'\n",
    "\n",
    "Dataset_store = 'Run_logging/cdnet/'\n",
    "if 1-os.path.exists(Dataset_store+part):\n",
    "    os.mkdir(Dataset_store+part)\n",
    "if 1-os.path.exists(Dataset_store+part+subpart):\n",
    "    os.mkdir(Dataset_store+part+subpart)\n",
    "    \n",
    "Dataset_name = Dataset_store+part+subpart\n",
    "DATA_PATH = '/home/amax/yyq/Dataset/CDnet2014/change_dataset/'+part+subpart\n",
    "t_label_path = DATA_PATH+'/total/t_label/'\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1,0'\n",
    "device1 = torch.device(\"cuda:0\")\n",
    "device0 = torch.device(\"cuda:1\")\n",
    "device2 = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef862d80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = os.path.join(DATA_PATH)\n",
    "TRAIN_LABEL_PATH = os.path.join(DATA_PATH)\n",
    "TRAIN_TXT_PATH = os.path.join(TRAIN_DATA_PATH,'train.txt')\n",
    "TEST_DATA_PATH = os.path.join(DATA_PATH)\n",
    "TEST_LABEL_PATH = os.path.join(DATA_PATH)\n",
    "TEST_TXT_PATH = os.path.join(TEST_DATA_PATH, 'test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813aa9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 2\n",
    "test_batch_size = val_batch_size = 2\n",
    "\n",
    "train_data = d.Dataset(TRAIN_DATA_PATH, TRAIN_LABEL_PATH,\n",
    "                            TRAIN_TXT_PATH,'train',transform=True)\n",
    "train_loader = DataLoader(train_data, batch_size=train_batch_size,\n",
    "                             shuffle= True, num_workers= 8, pin_memory= True)\n",
    "test_data = d.Dataset(TEST_DATA_PATH, TEST_LABEL_PATH,\n",
    "                        TEST_TXT_PATH,'test', transform=False)\n",
    "test_loader = DataLoader(test_data, batch_size=test_batch_size,\n",
    "                            shuffle= False, num_workers= 8, pin_memory= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1335f6bc-28b5-4d26-9852-5f09d1f1a332",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot(x,**kwargs):\n",
    "    plt.figure(dpi=100)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(x,**kwargs)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dea325",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_model(data_loader):\n",
    "    global best_acc\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        Accuracies = []\n",
    "        Right = 0\n",
    "        Sum = 0\n",
    "\n",
    "        TP = 0\n",
    "        FP = 0\n",
    "        FN = 0\n",
    "\n",
    "        for i,(i1,i2,label,file_name,mask) in enumerate(tqdm(data_loader)):\n",
    "            i1,i2,label = i1.cuda(),i2.cuda(),label.cuda()\n",
    "\n",
    "            prediction,c1,c2 = model(i1,i2)\n",
    "            prediction = (prediction>0.5).int()\n",
    "\n",
    "            Right += torch.sum(prediction == label)\n",
    "            Sum += torch.sum(label>-1)\n",
    "\n",
    "            impred = prediction\n",
    "            imlabel = label\n",
    "\n",
    "            accuracy = OA(prediction.view(-1),label.view(-1))\n",
    "            Accuracies.append(float(accuracy))\n",
    "\n",
    "            #Precision,recall,Iou\n",
    "            numclass = 1\n",
    "            TP +=  int(torch.sum(impred * (impred == imlabel)))\n",
    "            FP += int(torch.sum(impred * (impred != imlabel)))\n",
    "            FN += int(torch.sum(imlabel * (impred != imlabel)))\n",
    "\n",
    "        Average_accuracy = np.mean(Accuracies)\n",
    "        Overrall_accuracy = float(Right/Sum)\n",
    "        Iou = TP/(TP+FP+FN)\n",
    "        Percison = TP/(TP+FP)\n",
    "        Recall = TP/(TP+FN)\n",
    "        F1 = (2*Percison*Recall)/(Percison+Recall)\n",
    "\n",
    "        Average_accuracy = round(Average_accuracy,4)\n",
    "        Overrall_accuracy = round(Overrall_accuracy,4)\n",
    "        Iou = round(Iou,4)\n",
    "        Percison = round(Percison,4)\n",
    "        Recall = round(Recall,4)\n",
    "        F1 = round(F1,4)\n",
    "\n",
    "        print('AA: \\t\\t',Average_accuracy)\n",
    "        print('OA:\\t\\t',Overrall_accuracy)\n",
    "        print('Iou:\\t\\t',Iou)\n",
    "        print('Percison:\\t',Percison)\n",
    "        print('Recall:\\t\\t',Recall)\n",
    "        print('F1\\t\\t',F1)\n",
    "    \n",
    "    f = open(Log_path+'Metric_recording.txt','a')\n",
    "    f.write('Epoch:'+str(epoch)+'   Currentoa:'+str(round(F1,4))+'   Bestoa:'+str(round(best_acc,4))+'\\n')\n",
    "    f.close()\n",
    "    state_dict = Load_Weight_FordataParallel(model.state_dict(),need_dataparallel=0)\n",
    "    torch.save(state_dict, Log_path+'Last_model.pth')\n",
    "    \n",
    "    if F1 >= best_acc:\n",
    "        best_acc = F1\n",
    "        state_dict = Load_Weight_FordataParallel(model.state_dict(),need_dataparallel=0)\n",
    "        torch.save(state_dict, Log_path+'Best_model.pth')\n",
    "        f = open(Log_path+'Metric_recording.txt','a')\n",
    "        f.write('The details of performance: \\n')\n",
    "        f.write('AA: \\t\\t'+str(Average_accuracy)+'\\n'+'OA:\\t\\t'+str(Overrall_accuracy)+'\\n'+'Iou:\\t\\t'+str(Iou)+'\\n'+'Percison:\\t'+str(Percison)+'\\n'+'Recall:\\t\\t'+str(Recall)+'\\n'+'F1\\t\\t'+str(F1)+'\\n\\n')\n",
    "        f.close()\n",
    "    \n",
    "def eval_model():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        Accuracies = []\n",
    "        Right = 0\n",
    "        Sum = 0\n",
    "\n",
    "        TP = 0\n",
    "        FP = 0\n",
    "        FN = 0\n",
    "\n",
    "        for i,(i1,i2,label,file_name,mask) in enumerate(tqdm(test_loader)):\n",
    "            i1,i2,label = i1.cuda(),i2.cuda(),label.cuda()\n",
    "\n",
    "            prediction,c1,c2 = model(i1,i2)\n",
    "            prediction = (prediction>0.5).int()\n",
    "            \n",
    "            Right += torch.sum(prediction == label)\n",
    "            Sum += torch.sum(label>-1)\n",
    "\n",
    "            impred = prediction\n",
    "            imlabel = label\n",
    "\n",
    "            accuracy = OA(prediction.view(-1),label.view(-1))\n",
    "            Accuracies.append(float(accuracy))\n",
    "\n",
    "            #Precision,recall,Iou\n",
    "            TP +=  int(torch.sum(impred * (impred == imlabel)))\n",
    "            FP += int(torch.sum(impred * (impred != imlabel)))\n",
    "            FN += int(torch.sum(imlabel * (impred != imlabel)))\n",
    "\n",
    "        Average_accuracy = np.mean(Accuracies)\n",
    "        Overrall_accuracy = float(Right/Sum)\n",
    "        Iou = TP/(TP+FP+FN)\n",
    "        Percison = TP/(TP+FP)\n",
    "        Recall = TP/(TP+FN)\n",
    "        F1 = (2*Percison*Recall)/(Percison+Recall)\n",
    "\n",
    "        Average_accuracy = round(Average_accuracy,4)\n",
    "        Overrall_accuracy = round(Overrall_accuracy,4)\n",
    "        Iou = round(Iou,4)\n",
    "        Percison = round(Percison,4)\n",
    "        Recall = round(Recall,4)\n",
    "        F1 = round(F1,4)\n",
    "\n",
    "    print('AA: \\t\\t',Average_accuracy)\n",
    "    print('OA:\\t\\t',Overrall_accuracy)\n",
    "    print('Iou:\\t\\t',Iou)\n",
    "    print('Percison:\\t',Percison)\n",
    "    print('Recall:\\t\\t',Recall)\n",
    "    print('F1\\t\\t',F1)\n",
    "    \n",
    "def trend_test():\n",
    "    test_data = d.Dataset(TEST_DATA_PATH, TEST_LABEL_PATH,\n",
    "                            TEST_TXT_PATH,'test', transform=False)\n",
    "    test_loader = DataLoader(test_data, batch_size=1,\n",
    "                                shuffle= False, num_workers= 8, pin_memory= True)\n",
    "\n",
    "    TPs = [0,0,0]\n",
    "    FPs = [0,0,0]\n",
    "    FNs = [0,0,0]\n",
    "    Ts = [0,0,0]\n",
    "    Sums = [0,0,0]\n",
    "\n",
    "    model.eval()\n",
    "    for iter_, (i1,i2,label,file_name,mask) in enumerate(tqdm(test_loader)):\n",
    "        i1,i2,label = i1.cuda(),i2.cuda(),label.cuda()\n",
    "        with torch.no_grad():\n",
    "            p,c1,c2 = model(i1,i2)\n",
    "            c1_,c2_ = torch.max(c1,dim=1)[1],torch.max(c2,dim=1)[1]\n",
    "            t1_semantic, t2_semantic = c1_[0], c2_[0]\n",
    "            appear = ((t2_semantic-t1_semantic)==t2_semantic)*(t2_semantic!=0).int()\n",
    "            disappear = ((t1_semantic-t2_semantic)==t1_semantic)*(t1_semantic!=0).int()\n",
    "            transform = ((t2_semantic!=0).int())*((t1_semantic!=0).int())\n",
    "            trend_map = (appear+disappear*2+transform*3).cpu()\n",
    "\n",
    "            trend_name = file_name[0].split('/')[-1].split('.')[0]+'.npy'\n",
    "            label_trend_path = os.path.join(t_label_path,trend_name)\n",
    "            label_trend = torch.tensor(np.load(label_trend_path))\n",
    "\n",
    "            for i,value in enumerate([1,2,3]):\n",
    "                trend_map_i = (trend_map==value).int()\n",
    "                label_trend_i = (label_trend==value).int()\n",
    "                TPs[i]+= int(torch.sum(trend_map_i * (trend_map_i == label_trend_i)))\n",
    "                FPs[i]+= int(torch.sum(trend_map_i * (trend_map_i != label_trend_i)))\n",
    "                FNs[i]+= int(torch.sum(label_trend_i * (trend_map_i != label_trend_i)))\n",
    "                Ts[i] += int(torch.sum(trend_map_i == label_trend_i))\n",
    "                Sums[i]+=int(torch.sum(trend_map_i >-1))\n",
    "    OAs = []\n",
    "    IoUs = []\n",
    "    Ps = []\n",
    "    Rs = []\n",
    "    F1s = []\n",
    "    for i in range(3):\n",
    "        TP = TPs[i]\n",
    "        FP = FPs[i]\n",
    "        FN = FNs[i]\n",
    "        OAs.append(Ts[i]/Sums[i])\n",
    "        IoUs.append(TP/(TP+FP+FN))\n",
    "        p = TP/(TP+FP)\n",
    "        r = TP/(TP+FN)\n",
    "        Ps.append(p)\n",
    "        Rs.append(r)\n",
    "        F1s.append((2*p*r)/(p+r))\n",
    "    f = open(Log_path+'Final_performance.txt','a')\n",
    "    f.write('OA:'+str(OAs)+'\\nIoU:'+str(IoUs)+'\\nPrecision:'+str(Ps)+'\\nRecall:'+str(Rs)+'\\nF1:'+str(F1s)+'\\n')\n",
    "    f.close()\n",
    "\n",
    "\n",
    "    print(Ps)\n",
    "    print(Rs)\n",
    "    print(F1s)\n",
    "    print(IoUs)\n",
    "    print(OAs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ca6dd5-a308-492b-b14a-238bf96fd03e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "import torch\n",
    "\n",
    "def OA(pre_classes, gt_classes):\n",
    "    return torch.sum((pre_classes) == (gt_classes)).float()/len(pre_classes)\n",
    "\n",
    "def T_softmax(x,dim=1,T=0.1):\n",
    "    x = x/T\n",
    "    x_ = torch.exp(x)\n",
    "    x = x_/torch.sum(x_,dim=1).unsqueeze(dim)\n",
    "    return x\n",
    "\n",
    "from collections import OrderedDict\n",
    "def Load_Weight_FordataParallel(state_dict, need_dataparallel=0):\n",
    "        if_dataparallel = 1\n",
    "        for k, v in state_dict.items():\n",
    "            name = k[:6]\n",
    "            if name != \"module\":\n",
    "                if_dataparallel = 0\n",
    "        if need_dataparallel == 1:\n",
    "            if if_dataparallel == 1:\n",
    "                return state_dict\n",
    "            else:\n",
    "                new_state_dict = OrderedDict()\n",
    "                for k, v in state_dict.items():\n",
    "                    name = \"module.\"+k \n",
    "                    new_state_dict[name] = v \n",
    "                return new_state_dict\n",
    "        else:\n",
    "            if if_dataparallel == 0:\n",
    "                return state_dict\n",
    "            else:\n",
    "                new_state_dict = OrderedDict()\n",
    "                for k, v in state_dict.items():\n",
    "                    name = k[7:] \n",
    "                    new_state_dict[name] = v \n",
    "                return new_state_dict \n",
    "\n",
    "class conv_block(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolution Block \n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(conv_block, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class up_conv(nn.Module):\n",
    "    \"\"\"\n",
    "    Up Convolution Block\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(up_conv, self).__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, size=None):\n",
    "        if size is not None:\n",
    "            x = nn.Upsample(size=size, mode='bilinear')(x)\n",
    "        else:\n",
    "            x = nn.Upsample(scale_factor=2, mode='bilinear')(x)\n",
    "        x = self.up(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UNet_Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    UNet - Basic Implementation\n",
    "    Paper : https://arxiv.org/abs/1505.04597\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch=3, nl=32):\n",
    "        super(UNet_Encoder, self).__init__()\n",
    "\n",
    "        n1 = nl\n",
    "        filters = [n1, n1 * 2, n1 * 4, n1 * 8, n1 * 16]\n",
    "        \n",
    "        self.Maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.Maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.Maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.Maxpool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.Conv1 = conv_block(in_ch, filters[0])\n",
    "        self.Conv2 = conv_block(filters[0], filters[1])\n",
    "        self.Conv3 = conv_block(filters[1], filters[2])\n",
    "        self.Conv4 = conv_block(filters[2], filters[3])\n",
    "        self.Conv5 = conv_block(filters[3], filters[4])\n",
    "\n",
    "    def forward(self, x):\n",
    "        es = {}\n",
    "        \n",
    "        e1 = self.Conv1(x)\n",
    "        es['e1']=e1\n",
    "        \n",
    "        e2 = self.Maxpool1(e1)\n",
    "        e2 = self.Conv2(e2)\n",
    "        es['e2']=e2\n",
    "\n",
    "        e3 = self.Maxpool2(e2)\n",
    "        e3 = self.Conv3(e3)\n",
    "        es['e3']=e3\n",
    "\n",
    "        e4 = self.Maxpool3(e3)\n",
    "        e4 = self.Conv4(e4)\n",
    "        es['e4']=e4\n",
    "\n",
    "        e5 = self.Maxpool4(e4)\n",
    "        e5 = self.Conv5(e5)\n",
    "        es['e5']=e5\n",
    "\n",
    "        return es\n",
    "\n",
    "class UNet_Decoder_I(nn.Module):\n",
    "    def __init__(self, out_ch=2, nl = 64):\n",
    "        super(UNet_Decoder_I, self).__init__()\n",
    "\n",
    "        n1 = nl\n",
    "        filters = [n1, n1 * 2, n1 * 4, n1 * 8, n1 * 16]\n",
    "        \n",
    "        self.Up5 = up_conv(filters[4], filters[3])\n",
    "        self.Up_conv5 = conv_block(filters[4], filters[3])\n",
    "\n",
    "        self.Up4 = up_conv(filters[3], filters[2])\n",
    "        self.Up_conv4 = conv_block(filters[3], filters[2])\n",
    "\n",
    "        self.Up3 = up_conv(filters[2], filters[1])\n",
    "        self.Up_conv3 = conv_block(filters[2], filters[1])\n",
    "\n",
    "        self.Up2 = up_conv(filters[1], filters[0])\n",
    "        self.Up_conv2 = conv_block(filters[1], filters[0])\n",
    "\n",
    "    def forward(self, e):\n",
    "        e5 = e['e5']\n",
    "        e4 = e['e4']\n",
    "        e3 = e['e3']\n",
    "        e2 = e['e2']\n",
    "        e1 = e['e1']\n",
    "        \n",
    "        d5 = self.Up5(e5,size=e4.shape[2:])\n",
    "        d5 = torch.cat((e4, d5), dim=1)\n",
    "\n",
    "        d5 = self.Up_conv5(d5)\n",
    "\n",
    "        d4 = self.Up4(d5,size=e3.shape[2:])\n",
    "        d4 = torch.cat((e3, d4), dim=1)\n",
    "        d4 = self.Up_conv4(d4)\n",
    "\n",
    "        d3 = self.Up3(d4,size=e2.shape[2:])\n",
    "        d3 = torch.cat((e2, d3), dim=1)\n",
    "        d3 = self.Up_conv3(d3)\n",
    "\n",
    "        d2 = self.Up2(d3,size=e1.shape[2:])\n",
    "        d2 = torch.cat((e1, d2), dim=1)\n",
    "        d2 = self.Up_conv2(d2)\n",
    "\n",
    "        return d2\n",
    "\n",
    "\n",
    "class UNet_Decoder_S(nn.Module):\n",
    "    def __init__(self, out_ch=2, nl = 128):\n",
    "        super(UNet_Decoder_S, self).__init__()\n",
    "\n",
    "        n1 = nl\n",
    "        filters = [n1, n1 * 2, n1 * 4, n1 * 8, n1 * 16]\n",
    "        \n",
    "        self.Up5 = up_conv(filters[4], filters[3])\n",
    "        self.Up_conv5 = conv_block(filters[4], filters[3])\n",
    "\n",
    "        self.Up4 = up_conv(filters[3], filters[2])\n",
    "        self.Up_conv4 = conv_block(filters[3], filters[2])\n",
    "\n",
    "        self.Up3 = up_conv(filters[2], filters[1])\n",
    "        self.Up_conv3 = conv_block(filters[2], filters[1])\n",
    "\n",
    "        self.Up2 = up_conv(filters[1], filters[0])\n",
    "        self.Up_conv2 = conv_block(filters[1], filters[0])\n",
    "\n",
    "    def forward(self, t1_e, t2_e):\n",
    "        e5 = torch.cat((t1_e['e5'], t2_e['e5']),dim=1)\n",
    "        e4 = torch.cat((t1_e['e4'], t2_e['e4']),dim=1)\n",
    "        e3 = torch.cat((t1_e['e3'], t2_e['e3']),dim=1)\n",
    "        e2 = torch.cat((t1_e['e2'], t2_e['e2']),dim=1)\n",
    "        e1 = torch.cat((t1_e['e1'], t2_e['e1']),dim=1)\n",
    "        \n",
    "        d5 = self.Up5(e5,size=e4.shape[2:])\n",
    "        d5 = torch.cat((e4, d5), dim=1)\n",
    "\n",
    "        d5 = self.Up_conv5(d5)\n",
    "\n",
    "        d4 = self.Up4(d5,size=e3.shape[2:])\n",
    "        d4 = torch.cat((e3, d4), dim=1)\n",
    "        d4 = self.Up_conv4(d4)\n",
    "\n",
    "        d3 = self.Up3(d4,size=e2.shape[2:])\n",
    "        d3 = torch.cat((e2, d3), dim=1)\n",
    "        d3 = self.Up_conv3(d3)\n",
    "\n",
    "        d2 = self.Up2(d3,size=e1.shape[2:])\n",
    "        d2 = torch.cat((e1, d2), dim=1)\n",
    "        d2 = self.Up_conv2(d2)\n",
    "\n",
    "        return d2\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_ch=3, out_ch=2, nl=64, trend_num = 3, T=0.1):\n",
    "        super(UNet, self).__init__()\n",
    "        self.encoder = UNet_Encoder(in_ch,nl)\n",
    "        self.decoder_s = UNet_Decoder_S(out_ch,nl=nl*2)\n",
    "        self.decoder_i = UNet_Decoder_I(out_ch,nl=nl)\n",
    "        self.conv_s1 = nn.Conv2d(nl+nl*2, trend_num, kernel_size=1)\n",
    "        self.conv_s2 = nn.Conv2d(nl+nl*2, trend_num, kernel_size=1)\n",
    "        self.conv_i1 = nn.Conv2d(nl, trend_num, kernel_size=1)\n",
    "        self.conv_i2 = nn.Conv2d(nl, trend_num, kernel_size=1)\n",
    "        self.T = T\n",
    "    def Normalization(self, x,dim=1):\n",
    "        min_ = torch.min(x,dim=dim)[0].unsqueeze(dim)\n",
    "        max_ = torch.max(x,dim=dim)[0].unsqueeze(dim)\n",
    "        x = (x-min_)/(max_-min_)\n",
    "        return x\n",
    "\n",
    "    def forward(self, t1_i,t2_i):\n",
    "        t1_f = self.encoder(t1_i)\n",
    "        t2_f = self.encoder(t2_i)\n",
    "        \n",
    "        t1_s = self.decoder_i(t1_f)\n",
    "        t2_s = self.decoder_i(t2_f)\n",
    "        s = self.decoder_s(t1_f,t2_f)\n",
    "        \n",
    "        t1_trend_ = self.conv_s1(torch.cat((s,t1_s),dim=1))\n",
    "        t2_trend_ = self.conv_s2(torch.cat((s,t2_s),dim=1))\n",
    "        t1_trend_ = t1_trend_ - torch.max(t1_trend_,dim=1)[0].unsqueeze(1)\n",
    "        t2_trend_ = t2_trend_ - torch.max(t2_trend_,dim=1)[0].unsqueeze(1)\n",
    "\n",
    "        t1_trend = T_softmax(t1_trend_,dim=1,T=self.T)\n",
    "        t2_trend = T_softmax(t2_trend_,dim=1,T=self.T)\n",
    "        \n",
    "        p = 1-torch.sum(t1_trend*t2_trend,dim=1)\n",
    "        \n",
    "        t1_trend_ = self.conv_i1(t1_s)\n",
    "        t2_trend_ = self.conv_i2(t2_s)\n",
    "        t1_trend_ = t1_trend_ - torch.max(t1_trend_,dim=1)[0].unsqueeze(1)\n",
    "        t2_trend_ = t2_trend_ - torch.max(t2_trend_,dim=1)[0].unsqueeze(1)\n",
    "\n",
    "        t1_trend = T_softmax(t1_trend_,dim=1,T=self.T)\n",
    "        t2_trend = T_softmax(t2_trend_,dim=1,T=self.T)\n",
    "        \n",
    "        return p, t1_trend, t2_trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fa2e68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epoch_num = 99\n",
    "trend_num = 3\n",
    "best_acc = 0\n",
    "loss_mean = []\n",
    "pretrained_path = None\n",
    "# \n",
    "Log_path = Dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344c5d3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = UNet(3,2,trend_num=trend_num)\n",
    "model = nn.DataParallel(model)\n",
    "model = model.cuda()\n",
    "\n",
    "if pretrained_path is not None:\n",
    "    state_dict = Load_Weight_FordataParallel(torch.load(pretrained_path),need_dataparallel=1)\n",
    "    pretrain_state_dict = {}\n",
    "    for k,v in model.state_dict().items():\n",
    "        if k in state_dict.keys():\n",
    "            pretrain_state_dict[k] = state_dict[k]\n",
    "        else:\n",
    "            pretrain_state_dict[k] = v\n",
    "    model.load_state_dict(pretrain_state_dict)\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(),lr=1e-4)\n",
    "\n",
    "STEPS_PER_EPOCH = len(train_loader)\n",
    "TOTAL_STEPS = STEPS_PER_EPOCH * 80\n",
    "scheduler = lr_scheduler.StepLR(opt, step_size=TOTAL_STEPS, gamma=0.1)\n",
    "\n",
    "Loss_function_classify = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e384c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_p_n(p,label, weight=None):\n",
    "    loss_p = torch.mean(-label*torch.log(p+1e-10),dim=(1,2))\n",
    "    loss_n = torch.mean(-(1-label)*torch.log(1-p+1e-10),dim=(1,2))\n",
    "    if weight is not None:\n",
    "        loss = loss_p*(1-weight) + loss_n*weight\n",
    "    else:\n",
    "        loss = loss_p+loss_n\n",
    "    return torch.mean(loss)\n",
    "\n",
    "def loss_p(p,label,weight=None):\n",
    "    loss_p = torch.mean(-label*torch.log(p+1e-10),dim=(1,2))\n",
    "    if weight is not None:\n",
    "        loss_p = loss_p*weight\n",
    "    return torch.mean(loss_p)\n",
    "\n",
    "def sigmoid_coe(x,T=0.01):\n",
    "    y = 1/(1+torch.exp((-x+0.5)/T))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f778f28-ee27-4dd0-8e36-12c290066c26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_sum = []\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "    loss_mean = []\n",
    "    \n",
    "    loss_mean_1 = []\n",
    "    loss_mean_2 = []\n",
    "    for iter_, (i1,i2,label,file_name,mask) in enumerate(tqdm(train_loader)):\n",
    "        i1,i2,label = i1.cuda(),i2.cuda(),label.cuda()\n",
    "        p,c1,c2 = model(i1,i2)\n",
    "        nc1, nc2 = c1[:,0],c2[:,0]\n",
    "        ratio = torch.sum(label,dim=(1,2))/(label.shape[1]*label.shape[2])\n",
    "\n",
    "        loss1 = loss_p_n(p,label)\n",
    "        \n",
    "        loss2_n = loss_p(nc1*nc2, (1-label))\n",
    "        loss2_g = loss_p_n(1-torch.sum(c1*c2,dim=1),label)\n",
    "        loss2 = loss2_n+loss2_g\n",
    "\n",
    "        loss = (loss1+loss2)*10\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        loss_mean.append(loss.item())\n",
    "        loss_mean_1.append(loss1.item())\n",
    "        loss_mean_2.append(loss2.item())\n",
    "                \n",
    "        if (iter_+1)%10 ==0:\n",
    "            print('******************')\n",
    "            print()\n",
    "            print('Total loss: ',round(np.mean(loss_mean),5))\n",
    "            print('Change loss: ',round(np.mean(loss_mean_1),5))\n",
    "            print('Trend loss: ',round(np.mean(loss_mean_2),5))\n",
    "            print('******************')\n",
    "            loss_sum.append(np.mean(loss_mean))\n",
    "            loss_mean = []\n",
    "            loss_mean_1 = []\n",
    "            loss_mean_2 = []\n",
    "            loss_mean_3 = []\n",
    "    try:\n",
    "        trend_test()\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a796da4-3c3d-4a06-bf3f-501811daaea4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d18261-16dd-4601-ba41-a773b7c2aec0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8144373-7d31-49e3-9851-30cfcd94b5a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bd65b2-31e9-4d11-a4ec-67325adb010b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_save(x,**kwargs):\n",
    "    plt.figure(dpi=100)\n",
    "    plt.axis('off')\n",
    "    plt.imsave(arr=x,**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7161517-6472-4bbc-bf98-a7698332b660",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "colors = ['black', 'deepskyblue', 'white','red'] \n",
    "cmap = mpl.colors.ListedColormap(colors)\n",
    "\n",
    "test_batch_size=1\n",
    "\n",
    "test_data = d.Dataset(TRAIN_DATA_PATH, TEST_LABEL_PATH,\n",
    "                        TEST_TXT_PATH,'test', transform=False)\n",
    "test_loader = DataLoader(test_data, batch_size=test_batch_size,\n",
    "                            shuffle= False, num_workers= 8, pin_memory= True)\n",
    "\n",
    "train_data = d.Dataset(TRAIN_DATA_PATH, TRAIN_LABEL_PATH,\n",
    "                            TRAIN_TXT_PATH,'train',transform=False)\n",
    "train_loader = DataLoader(train_data, batch_size=test_batch_size,\n",
    "                             shuffle= True, num_workers= 8, pin_memory= True)\n",
    "\n",
    "for iter_, (i1,i2,label,file_name,mask) in enumerate(tqdm(test_loader)):\n",
    "    i1,i2,label = i1.cuda(),i2.cuda(),label.cuda()\n",
    "    with torch.no_grad():\n",
    "        p,c1,c2 = model(i1,i2)\n",
    "        c1_,c2_ = torch.max(c1,dim=1)[1],torch.max(c2,dim=1)[1]\n",
    "    \n",
    "    index=0\n",
    "    save_path = Log_path+'save_pic/'+str(iter_)+'/'\n",
    "    if 1-os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    \n",
    "    i_name = file_name[index].split('/')[-1]\n",
    "    print(i_name)\n",
    "\n",
    "    I1 = i1[index].permute(1,2,0).cpu()+0.5\n",
    "    I2 = i2[index].permute(1,2,0).cpu()+0.5\n",
    "    Label = label[index].detach().cpu()\n",
    "\n",
    "\n",
    "    t1_semantic, t2_semantic = c1_[index], c2_[index]\n",
    "\n",
    "    appear = ((t2_semantic-t1_semantic)==t2_semantic)*(t2_semantic!=0).int()\n",
    "    disappear = ((t1_semantic-t2_semantic)==t1_semantic)*(t1_semantic!=0).int()\n",
    "    transform = ((t2_semantic!=0).int())*((t1_semantic!=0).int())\n",
    "    trend_map = (appear+disappear*2+transform*3).cpu().numpy()\n",
    "    \n",
    "    print('predicted_trend_map')\n",
    "    plot_save(trend_map,vmin=0,vmax=trend_num,cmap=cmap,fname=save_path+'Trend_P.png')\n",
    "    \n",
    "    trend_name = file_name[0].split('/')[-1].split('.')[0]+'.npy'\n",
    "    label_trend_path = os.path.join(t_label_path,trend_name)\n",
    "    label_trend = np.load(label_trend_path)\n",
    "    \n",
    "    print('label_trend_map')\n",
    "    plot_save(label_trend,vmin=0,vmax=trend_num,cmap=cmap,fname=save_path+'Trend_GT.png')\n",
    "\n",
    "    plot_save(i1[index].permute(1,2,0).cpu().numpy()+0.5,fname=save_path+'T1.png')\n",
    "    plot_save(i2[index].permute(1,2,0).cpu().numpy()+0.5,fname=save_path+'T2.png')\n",
    "    plot_save((p[index].detach().cpu().numpy()>0.5).astype(int),fname=save_path+'P.png')\n",
    "\n",
    "    print(\"label\")\n",
    "    plot_save(label[index].detach().cpu().numpy(),cmap='gray',fname=save_path+'GT.png')\n",
    "              \n",
    "    print('**********')\n",
    "    plot_save(torch.sum(c1[index,1:],dim=0).detach().cpu().numpy(),fname=save_path+'Trend_T1.png')\n",
    "    plot_save(torch.sum(c2[index,1:],dim=0).detach().cpu().numpy(),fname=save_path+'Trend_T2.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
